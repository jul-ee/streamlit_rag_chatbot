"""
streamlit_rag_upload.py
-----------------------
ì—…ë¡œë“œëœ PDF 1ê°œë¥¼ ëŒ€ìƒìœ¼ë¡œ ì‹¤ì‹œê°„ RAG ì§ˆì˜ì‘ë‹µì„ ì œê³µí•˜ëŠ” RAG ì±—ë´‡

â€¢ secrets.tomlì—ì„œ OPENAI_API_KEY ë¡œë“œ
â€¢ PDF â†’ í˜ì´ì§€ ë¶„í• (RecursiveCharacterTextSplitter) â†’ OpenAI ì„ë² ë”©
â€¢ Chroma ë²¡í„°ìŠ¤í† ì–´ì— ì„ë² ë”© ì €ì¥(ë©”ëª¨ë¦¬ ê¸°ë°˜, ìºì‹±)
â€¢ LangChain RAG ì²´ì¸(ChatPromptTemplate + GPT-4o)ìœ¼ë¡œ ë‹µë³€ ìƒì„±
â€¢ Streamlit chat UI + ì„¸ì…˜ ìƒíƒœë¡œ ëŒ€í™” ê¸°ë¡ ê´€ë¦¬
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â… . Import & í™˜ê²½ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os
import streamlit as st
import tempfile

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# Chroma ë‹¤ì¤‘ í…Œë„ŒíŠ¸ ìºì‹œ ì˜¤ë¥˜ ë°©ì§€
import chromadb
chromadb.api.client.SharedSystemClient.clear_system_cache()

# secrets.toml â†’ ëŸ°íƒ€ì„ìœ¼ë¡œ API í‚¤ ë¡œë“œ
OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]

st.set_page_config(page_title="ChatPDF", page_icon="ğŸ“„")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â…¡. ë°ì´í„° ì¤€ë¹„ / ë²¡í„°ìŠ¤í† ì–´
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
def load_pdf(_file):
    """UploadedFile â–¶ ì„ì‹œíŒŒì¼ â–¶ LangChain Documents ë°˜í™˜"""
    with tempfile.NamedTemporaryFile(mode="wb", delete=False) as tmp_file:
        tmp_file.write(_file.getvalue())
        tmp_file_path = tmp_file.name
        loader = PyPDFLoader(file_path=tmp_file_path)
        pages = loader.load_and_split()
    return pages


@st.cache_resource
def create_vector_store(_docs):
    """ë¬¸ì„œ ì²­í¬ â†’ OpenAI ì„ë² ë”© â†’ Chroma ë²¡í„°ìŠ¤í† ì–´"""
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    split_docs = text_splitter.split_documents(_docs)
    vectorstore = Chroma.from_documents(
        split_docs,
        OpenAIEmbeddings(
            model="text-embedding-3-small",
            openai_api_key=OPENAI_API_KEY
        )
    )
    return vectorstore


def format_docs(docs):
    """ê²€ìƒ‰ëœ Document â†’ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë³‘í•©"""
    return "\n\n".join(doc.page_content for doc in docs)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â…¢. LangChain RAG ì²´ì¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
def chaining(_pages):
    """ì—…ë¡œë“œëœ PDF ê¸°ë°˜ RAG ì²´ì¸ ìƒì„±"""
    vectorstore = create_vector_store(_pages)
    retriever = vectorstore.as_retriever()

    qa_system_prompt = """
    You are an assistant for question-answering tasks. \
    Use the following pieces of retrieved context to answer the question. \
    If you don't know the answer, just say that you don't know. \
    Keep the answer perfect. please use imogi with the answer. \
    Please answer in Korean and use respectful language.\
    {context}
    """
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            ("human", "{input}"),
        ]
    )

    llm = ChatOpenAI(
        model="gpt-4o",
        openai_api_key=OPENAI_API_KEY
    )

    rag_chain = (
        {"context": retriever | format_docs, "input": RunnablePassthrough()}
        | qa_prompt
        | llm
        | StrOutputParser()
    )
    return rag_chain

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â…£. Streamlit UI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.header("ğŸ’¬ ChatPDF")

uploaded_file = st.file_uploader("Upload a PDF", type=["pdf"])

if uploaded_file is not None:
    pages = load_pdf(uploaded_file)
    rag_chain = chaining(pages)

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            {"role": "assistant", "content": "ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!"}
        ]

    # ê³¼ê±° ëŒ€í™” ë Œë”ë§
    for msg in st.session_state.messages:
        st.chat_message(msg['role']).write(msg['content'])

    # ì‚¬ìš©ì ì§ˆë¬¸ ì²˜ë¦¬
    if prompt_message := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš” :)"):
        st.chat_message("human").write(prompt_message)
        st.session_state.messages.append({"role": "user", "content": prompt_message})

        with st.chat_message("ai"):
            with st.spinner("Thinking..."):
                response = rag_chain.invoke(prompt_message)
                st.session_state.messages.append(
                    {"role": "assistant", "content": response}
                )
                st.write(response)
