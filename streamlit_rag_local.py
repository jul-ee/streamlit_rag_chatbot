"""
streamlit_rag_local.py
--------------------------------------
ëŒ€í•œë¯¼êµ­ í—Œë²• PDFë¥¼ ë²¡í„° DB(Chroma)ì— ì„ë² ë”©í•œ ë’¤
GPT-4o-mini ëª¨ë¸ë¡œ ì§ˆì˜ì‘ë‹µì„ ìˆ˜í–‰í•˜ëŠ” RAG ì±—ë´‡

â€¢ secrets.tomlì—ì„œ OPENAI_API_KEY ë¡œë“œ
â€¢ ìºì‹±(@st.cache_resource)ìœ¼ë¡œ PDF íŒŒì‹±Â·ë²¡í„°í™” ë¹„ìš© ì ˆê°
â€¢ Chroma ë²¡í„°ìŠ¤í† ì–´ê°€ ìˆìœ¼ë©´ ì¬ì‚¬ìš©, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
â€¢ Streamlit chat UI + ì„¸ì…˜ ìƒíƒœë¡œ ëŒ€í™” ê¸°ë¡ ìœ ì§€
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â… . Import & í™˜ê²½ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os
import streamlit as st

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# secrets.toml â†’ ëŸ°íƒ€ì„ìœ¼ë¡œ API í‚¤ ë¡œë“œ
OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â…¡. PDF ë¡œë“œ & ë²¡í„°ìŠ¤í† ì–´
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
def load_and_split_pdf(file_path):
    """PDF ê²½ë¡œ â†’ LangChain Document ë¦¬ìŠ¤íŠ¸"""
    loader = PyPDFLoader(file_path)
    return loader.load_and_split()


@st.cache_resource
def create_vector_store(_docs):
    """
    ë¬¸ì„œë¥¼ 1 000ì ë‹¨ìœ„ë¡œ ë¶„í• í•œ ë’¤
    OpenAI ì„ë² ë”© â†’ Chroma ë²¡í„° DBì— ì €ì¥
    """
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    split_docs = text_splitter.split_documents(_docs)

    vectorstore = Chroma.from_documents(
        split_docs,
        OpenAIEmbeddings(
            model="text-embedding-3-small",
            openai_api_key=OPENAI_API_KEY
        ),
        persist_directory="./chroma_db"
    )
    return vectorstore


@st.cache_resource
def get_vectorstore(_docs):
    """ê¸°ì¡´ Chroma DBê°€ ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±"""
    persist_directory = "./chroma_db"
    if os.path.exists(persist_directory):
        return Chroma(
            persist_directory=persist_directory,
            embedding_function=OpenAIEmbeddings(
                model="text-embedding-3-small",
                openai_api_key=OPENAI_API_KEY
            )
        )
    return create_vector_store(_docs)


def format_docs(docs):
    """ê²€ìƒ‰ëœ Document ë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹¨"""
    return "\n\n".join(doc.page_content for doc in docs)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â…¢. LangChain RAG ì²´ì¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
def chaining():
    """PDF ë¡œë“œ â†’ ë²¡í„°ìŠ¤í† ì–´ â†’ RAG ì²´ì¸ ì¡°ë¦½"""
    file_path = r"./data/ëŒ€í•œë¯¼êµ­í—Œë²•(í—Œë²•)(ì œ00010í˜¸)(19880225).pdf"
    pages = load_and_split_pdf(file_path)
    vectorstore = get_vectorstore(pages)
    retriever = vectorstore.as_retriever()

    qa_system_prompt = """
    You are an assistant for question-answering tasks.
    Use the following pieces of retrieved context to answer the question.
    If you don't know the answer, just say that you don't know.
    Keep the answer perfect. Please use emoji with the answer.
    Please answer in Korean and use respectful language.
    {context}
    """
    qa_prompt = ChatPromptTemplate.from_messages(
        [("system", qa_system_prompt), ("human", "{input}")]
    )

    llm = ChatOpenAI(
        model="gpt-4o-mini",
        openai_api_key=OPENAI_API_KEY    # API í‚¤ ì „ë‹¬
    )

    rag_chain = (
        {"context": retriever | format_docs, "input": RunnablePassthrough()}
        | qa_prompt
        | llm
        | StrOutputParser()
    )
    return rag_chain


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â…£. Streamlit UI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.header("í—Œë²• Q&A ì±—ë´‡ ğŸ’¬ ğŸ“š")
rag_chain = chaining()

# 1) ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "assistant", "content": "í—Œë²•ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!"}
    ]

# 2) ê³¼ê±° ëŒ€í™” ë Œë”ë§
for msg in st.session_state.messages:
    st.chat_message(msg['role']).write(msg['content'])

# 3) ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt_message := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš” :)"):
    st.chat_message("human").write(prompt_message)
    st.session_state.messages.append({"role": "user", "content": prompt_message})

    with st.chat_message("ai"):
        with st.spinner("Thinking..."):
            response = rag_chain.invoke(prompt_message)
            st.session_state.messages.append(
                {"role": "assistant", "content": response}
            )
            st.write(response)
